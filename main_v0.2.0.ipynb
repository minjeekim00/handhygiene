{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.handhygiene import HandHygiene\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from spatial_transforms import Compose\n",
    "from spatial_transforms import Normalize\n",
    "from spatial_transforms import Scale\n",
    "from spatial_transforms import CenterCrop\n",
    "from spatial_transforms import RandomHorizontalFlip\n",
    "from spatial_transforms import RandomAffine\n",
    "from spatial_transforms import RandomRotation\n",
    "from spatial_transforms import ColorJitter\n",
    "from spatial_transforms import ToTensor #ExtractSkinColor\n",
    "from temporal_transforms import TemporalRandomChoice\n",
    "from temporal_transforms import TemporalRandomCrop\n",
    "from temporal_transforms import LoopPadding, MirrorPadding, MirrorLoopPadding\n",
    "from openpose_transforms import MultiScaleTorsoRandomCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIR='./data/images' #/data/videos\n",
    "CLIP_LEN = 16\n",
    "CLIP_LEN_AUG = CLIP_LEN/2\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "IMG_SIZE = 224\n",
    "MEAN=[110.63666788, 103.16065604, 96.29023126]\n",
    "STD=[38.7568578, 37.88248729, 40.02898126]\n",
    "MEAN=[128, 128, 128]\n",
    "STD=[128, 128, 128]\n",
    "\n",
    "scales=np.linspace(1, 1.75, num=1e3)\n",
    "center=((1+1.75)/2)\n",
    "openpose_transform = {\n",
    "    'train':MultiScaleTorsoRandomCrop(scales, IMG_SIZE),\n",
    "    'val':MultiScaleTorsoRandomCrop(np.linspace(center, center, num=1), IMG_SIZE, centercrop=True)\n",
    "}\n",
    "\n",
    "spatial_transform = {\n",
    "    'train': Compose([Scale(IMG_SIZE),\n",
    "                      CenterCrop(IMG_SIZE),\n",
    "                      #RandomHorizontalFlip(),\n",
    "                      #ColorJitter(brightness=0.1),\n",
    "                      #RandomAffine(5),\n",
    "                      #RandomRotation(2.5),\n",
    "                      ToTensor(1), \n",
    "                      Normalize(MEAN, STD)]),\n",
    "    'val': Compose([Scale(IMG_SIZE), \n",
    "                    CenterCrop(IMG_SIZE), \n",
    "                    ToTensor(1), \n",
    "                    Normalize(MEAN, STD)])}\n",
    "\n",
    "temporal_transform = Compose([\n",
    "            TemporalRandomCrop(CLIP_LEN_AUG),\n",
    "            TemporalRandomChoice([\n",
    "                LoopPadding(CLIP_LEN),\n",
    "                MirrorPadding(CLIP_LEN),\n",
    "                MirrorLoopPadding(CLIP_LEN)])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:09<00:00,  3.02s/it]\n",
      "100%|██████████| 1/1 [00:06<00:00,  6.29s/it]\n",
      "100%|██████████| 1/1 [00:05<00:00,  5.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ./data/images/test people: 17\n"
     ]
    }
   ],
   "source": [
    "dataset_test = HandHygiene(os.path.join(VIDEO_DIR, 'test'), \n",
    "                        frames_per_clip=CLIP_LEN,\n",
    "                        openpose_transform=openpose_transform['val'],\n",
    "                        spatial_transform=spatial_transform['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': I3DDataset(os.path.join(VIDEO_DIR, 'train'), \n",
    "                          frames_per_clip=CLIP_LEN,\n",
    "                          #temporal_transform=temporal_transform,\n",
    "                          spatial_transform=spatial_transform['train']),\n",
    "    'val': I3DDataset(os.path.join(VIDEO_DIR, 'val'), \n",
    "                        frames_per_clip=CLIP_LEN,\n",
    "                        spatial_transform=spatial_transform['val']),\n",
    "    'test': I3DDataset(os.path.join(VIDEO_DIR, 'test'), \n",
    "                        frames_per_clip=CLIP_LEN,\n",
    "                        spatial_transform=spatial_transform['val'])\n",
    "}\n",
    "dataloaders = {\n",
    "    'train': DataLoader(dataset['train'], batch_size=BATCH_SIZE, shuffle=True, num_workers=16),\n",
    "    'val': DataLoader(dataset['val'], batch_size=BATCH_SIZE, shuffle=False, num_workers=16)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 480, 640, 3]) x 16\n",
      "torch.Size([15, 480, 640, 3]) x 16\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-30aaf93020f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mshow_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data/private/minjee-video/handhygiene/dataloader/handhygiene.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenpose_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             streams = [self.openpose_transform(c, f, rois, i)\n\u001b[0;32m---> 83\u001b[0;31m                        for i, (c, f) in enumerate(zip(video, optflow))]\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstreams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"windows empty\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/private/minjee-video/handhygiene/dataloader/handhygiene.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopenpose_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             streams = [self.openpose_transform(c, f, rois, i)\n\u001b[0;32m---> 83\u001b[0;31m                        for i, (c, f) in enumerate(zip(video, optflow))]\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstreams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"windows empty\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/private/minjee-video/handhygiene/openpose_transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img, flow, coords, index)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mwindows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"empty windows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/private/minjee-video/handhygiene/openpose_transforms.py\u001b[0m in \u001b[0;36mget_windows\u001b[0;34m(self, coords)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_windows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mpeople\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'people'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mtorso\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'torso'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mwindows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def show_dataset(item): # item: C x D x 244 x 244\n",
    "    rgbs = item[0].transpose(0, 1)\n",
    "    flows = item[1].transpose(0, 1)\n",
    "    n = len(rgbs)\n",
    "    shape = np.asarray(flows[0]).shape\n",
    "    rgb = np.hstack((np.asarray(rgb).transpose(1, 2, 0)+1)/2 for rgb in rgbs)\n",
    "    tmp = np.zeros((shape[1], shape[2], 1))\n",
    "    flow = np.hstack((np.dstack((np.asarray(flow).transpose(1, 2, 0), tmp))+1)/2 for flow in flows)\n",
    "\n",
    "    img = np.vstack((rgb, flow))\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(item[1])\n",
    "    \n",
    "i=200\n",
    "phase='val'\n",
    "show_dataset(dataset_test.__getitem__(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'i3d'\n",
    "batch_size = 16\n",
    "clip_len = 16\n",
    "sample_duration = clip_len\n",
    "num_classes = 1\n",
    "\n",
    "#torch.manual_seed(100)\n",
    "data_name = 'anesthesia'\n",
    "dataset_path = os.path.join(os.getcwd(), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 224\n",
    "#mean=[110.63666788, 103.16065604, 96.29023126]\n",
    "#std=[38.7568578, 37.88248729, 40.02898126]\n",
    "mean=[128, 128, 128]\n",
    "std=[128, 128, 128]\n",
    "\n",
    "scales=np.linspace(1, 1.75, num=1e3)\n",
    "center=((1+1.75)/2)\n",
    "openpose_transform = {\n",
    "    'train':MultiScaleTorsoRandomCrop(scales, sample_size),\n",
    "    'val':MultiScaleTorsoRandomCrop(np.linspace(center, center, num=1), sample_size, centercrop=True)\n",
    "}\n",
    "spatial_transform = {\n",
    "    'train': Compose([Scale(sample_size),\n",
    "                      CenterCrop(sample_size),\n",
    "                      RandomHorizontalFlip(),\n",
    "                      ColorJitter(brightness=0.1),\n",
    "                      RandomAffine(5),\n",
    "                      RandomRotation(2.5),\n",
    "                      ToTensor(1), \n",
    "                      Normalize(mean, std)]),\n",
    "    'val': Compose([Scale(sample_size), \n",
    "                    CenterCrop(sample_size), \n",
    "                    ToTensor(1), \n",
    "                    Normalize(mean, std)])}\n",
    "temporal_transform = {\n",
    "    'train': TemporalRandomChoice([\n",
    "            TemporalBeginCrop(sample_duration),\n",
    "            MirrorPadding(sample_duration)]),\n",
    "    'val':TemporalBeginCrop(sample_duration)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train':VideoFolder(dataset_path, split='train', clip_len=clip_len, \n",
    "                        spatial_transform=spatial_transform['train'],\n",
    "                        temporal_transform=temporal_transform['train']),\n",
    "    'val':VideoFolder(dataset_path, split='val', clip_len=clip_len, \n",
    "                        spatial_transform=spatial_transform['val'],\n",
    "                        temporal_transform=temporal_transform['val']),\n",
    "    'test':VideoFolder(dataset_path, split='test', clip_len=clip_len, \n",
    "                        spatial_transform=spatial_transform['val'],\n",
    "                        temporal_transform=temporal_transform['val'])}\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(dataset['train'], batch_size=batch_size, shuffle=True, num_workers=16),\n",
    "    'val': DataLoader(dataset['val'], batch_size=1, shuffle=False, num_workers=16)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST DATA SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# for path in sorted(glob('./data/videos/simulate/clean/*.mp4'))[-17:]:\n",
    "#     !mv $path /data/private/minjee-video/handhygiene/data/videos/simulate/test/clean\n",
    "for mod in ['images']:\n",
    "    ext = '.mp4' if mod == 'videos' else ''\n",
    "    for path in sorted(glob('./data/{}/simulate/notclean/*{}'.format(mod, ext)))[-10:]:\n",
    "        path = path.replace('videos', mod)\n",
    "        dst = '/data/private/minjee-video/handhygiene/data/{}/simulate/test/notclean'.format(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for mod in ['images']:\n",
    "    ext = '.mp4' if mod == 'videos' else ''\n",
    "    for label in ['notclean']:\n",
    "        VIDEOS = sorted(glob('./data/{}/simulate/{}/*{}'.format(mod, label, ext)))\n",
    "        #VIDEO_TRAIN, VIDEO_VAL = train_test_split(VIDEOS, test_size=0.2, random_state=42)\n",
    "        VIDEO_DATA = {'train': VIDEO_TRAIN, 'val': VIDEO_VAL}\n",
    "        for phase in ['train', 'val']:\n",
    "            paths = VIDEO_DATA[phase]\n",
    "            for path in paths:\n",
    "                path = path.replace('videos', mod)\n",
    "                dst = './data/{}/simulate/{}/{}/'.format(mod, phase, label)\n",
    "                !mv $path $dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_VAL = [os.path.splitext(path.replace('videos','images'))[0] for path in VIDEO_VAL]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
