{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.videodataset import VideoDataset\n",
    "from dataloader.handhygiene import HandHygiene\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from spatial_transforms import Compose\n",
    "from spatial_transforms import Normalize\n",
    "from spatial_transforms import Scale\n",
    "from spatial_transforms import CenterCrop\n",
    "from spatial_transforms import RandomHorizontalFlip\n",
    "from spatial_transforms import RandomAffine\n",
    "from spatial_transforms import RandomRotation\n",
    "from spatial_transforms import ColorJitter\n",
    "from spatial_transforms import ToTensor #ExtractSkinColor\n",
    "from temporal_transforms import TemporalRandomChoice\n",
    "from temporal_transforms import TemporalRandomCrop\n",
    "from temporal_transforms import LoopPadding, MirrorPadding, MirrorLoopPadding\n",
    "from openpose_transforms import MultiScaleTorsoRandomCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIR='./data/images' #/data/videos\n",
    "CLIP_LEN = 16\n",
    "CLIP_LEN_AUG = CLIP_LEN/2\n",
    "BATCH_SIZE = 16\n",
    "NUM_CLASSES=1\n",
    "NUM_WORKERS=16\n",
    "\n",
    "IMG_SIZE = 224\n",
    "MEAN=[110.63666788, 103.16065604, 96.29023126]\n",
    "STD=[38.7568578, 37.88248729, 40.02898126]\n",
    "#MEAN=[128, 128, 128]\n",
    "#STD=[128, 128, 128]\n",
    "\n",
    "scales=np.linspace(1, 1.75, num=1e3)\n",
    "center=((1+1.75)/2)\n",
    "openpose_transform = {\n",
    "    'train':MultiScaleTorsoRandomCrop(scales, IMG_SIZE),\n",
    "    'val':MultiScaleTorsoRandomCrop(np.linspace(center, center, num=1), IMG_SIZE, centercrop=True)\n",
    "}\n",
    "\n",
    "spatial_transform = {\n",
    "    'train': Compose([Scale(IMG_SIZE),\n",
    "                      CenterCrop(IMG_SIZE),\n",
    "                      RandomHorizontalFlip(),\n",
    "                      ColorJitter(brightness=0.1),\n",
    "                      RandomAffine(5),\n",
    "                      RandomRotation(2.5),\n",
    "                      ToTensor(1), \n",
    "                      Normalize(MEAN, STD)]),\n",
    "    'val': Compose([Scale(IMG_SIZE), \n",
    "                    CenterCrop(IMG_SIZE), \n",
    "                    ToTensor(1), \n",
    "                    Normalize(MEAN, STD)])}\n",
    "\n",
    "temporal_transform = {'train':Compose([\n",
    "                #TemporalRandomCrop(CLIP_LEN_AUG),\n",
    "                TemporalRandomChoice([\n",
    "                    LoopPadding(CLIP_LEN),\n",
    "                    MirrorPadding(CLIP_LEN),\n",
    "                    MirrorLoopPadding(CLIP_LEN)])]),\n",
    "                     'val':LoopPadding(CLIP_LEN)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train': HandHygiene(os.path.join(VIDEO_DIR, 'train'), \n",
    "                          frames_per_clip=CLIP_LEN,\n",
    "                          temporal_transform=temporal_transform['train'],\n",
    "                          openpose_transform=openpose_transform['train'],\n",
    "                         spatial_transform=spatial_transform['train']),\n",
    "    'val': HandHygiene(os.path.join(VIDEO_DIR, 'val'), \n",
    "                        frames_per_clip=CLIP_LEN,\n",
    "                          temporal_transform=temporal_transform['val'],\n",
    "                          openpose_transform=openpose_transform['val'],\n",
    "                        spatial_transform=spatial_transform['val']),\n",
    "    'test': HandHygiene(os.path.join(VIDEO_DIR, 'test'), \n",
    "                        frames_per_clip=CLIP_LEN,\n",
    "                          temporal_transform=temporal_transform['val'],\n",
    "                          openpose_transform=openpose_transform['val'],\n",
    "                        spatial_transform=spatial_transform['val'])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(dataset['train'], batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS),\n",
    "    'val': DataLoader(dataset['val'], batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def show_dataset(item): # item: C x D x 244 x 244\n",
    "    rgbs = item[0].transpose(0, 1)\n",
    "    flows = item[1].transpose(0, 1)\n",
    "    n = len(rgbs)\n",
    "    shape = np.asarray(flows[0]).shape\n",
    "    rgb = np.hstack((np.asarray(rgb).transpose(1, 2, 0)+1)/2 for rgb in rgbs)\n",
    "    tmp = np.zeros((shape[1], shape[2], 1))\n",
    "    flow = np.hstack((np.dstack((np.asarray(flow).transpose(1, 2, 0), tmp))+1)/2 for flow in flows)\n",
    "\n",
    "    img = np.vstack((rgb, flow))\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(item[2])\n",
    "    \n",
    "i=1\n",
    "phase='train'\n",
    "show_dataset(dataset[phase].__getitem__(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phase='train'\n",
    "flows_clean_mean=[]\n",
    "flows_clean_std=[]\n",
    "flows_notclean_mean=[]\n",
    "flows_notclean_std=[]\n",
    "for i in tqdm(range(dataset[phase].__len__())):\n",
    "    flow = dataset[phase][i][1]\n",
    "    vidx, _ = dataset[phase].video_clips.get_clip_location(i)\n",
    "    label = dataset[phase].samples[vidx][2]\n",
    "    if label == 0:\n",
    "        flows_clean_mean.append(torch.mean(flow))\n",
    "        flows_clean_std.append(torch.std(flow))\n",
    "    else:\n",
    "        flows_notclean_mean.append(torch.mean(flow))\n",
    "        flows_notclean_std.append(torch.std(flow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import get_models\n",
    "from train import train\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "i3d_rgb, i3d_flow = get_models(NUM_CLASSES, True, 170, load_pt_weights=True) # unfreeze last mix 170, 152\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    i3d_rgb = nn.DataParallel(i3d_rgb)\n",
    "    i3d_flow = nn.DataParallel(i3d_flow)\n",
    "i3d_rgb.to(device)\n",
    "i3d_flow.to(device)\n",
    "\n",
    "criterion = F.binary_cross_entropy\n",
    "optims={'rgb':None, 'flow':None}\n",
    "schedulers = {'rgb':None, 'flow':None}\n",
    "feature_extract=True\n",
    "\n",
    "def trainable_params(model, mode='rgb'):\n",
    "    params_to_update = model.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\",name)\n",
    "    else:\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(\"\\t\",name)\n",
    "    optims[mode] = optim.SGD(model.parameters(), lr=1e-6, momentum=0.9, weight_decay=1e-7)\n",
    "\n",
    "trainable_params(i3d_rgb, 'rgb')\n",
    "trainable_params(i3d_flow, 'flow')\n",
    "    \n",
    "schedulers['rgb'] = MultiStepLR(optims['rgb'], milestones=[10], gamma=0.1)\n",
    "schedulers['flow'] = MultiStepLR(optims['flow'], milestones=[10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train((i3d_rgb, i3d_flow), dataloaders, optims, criterion, schedulers, device, num_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST DATA SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# for path in sorted(glob('./data/videos/simulate/clean/*.mp4'))[-17:]:\n",
    "#     !mv $path /data/private/minjee-video/handhygiene/data/videos/simulate/test/clean\n",
    "for mod in ['images']:\n",
    "    ext = '.mp4' if mod == 'videos' else ''\n",
    "    for path in sorted(glob('./data/{}/simulate/notclean/*{}'.format(mod, ext)))[-10:]:\n",
    "        path = path.replace('videos', mod)\n",
    "        dst = '/data/private/minjee-video/handhygiene/data/{}/simulate/test/notclean'.format(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for mod in ['images']:\n",
    "    ext = '.mp4' if mod == 'videos' else ''\n",
    "    for label in ['notclean']:\n",
    "        VIDEOS = sorted(glob('./data/{}/simulate/{}/*{}'.format(mod, label, ext)))\n",
    "        #VIDEO_TRAIN, VIDEO_VAL = train_test_split(VIDEOS, test_size=0.2, random_state=42)\n",
    "        VIDEO_DATA = {'train': VIDEO_TRAIN, 'val': VIDEO_VAL}\n",
    "        for phase in ['train', 'val']:\n",
    "            paths = VIDEO_DATA[phase]\n",
    "            for path in paths:\n",
    "                path = path.replace('videos', mod)\n",
    "                dst = './data/{}/simulate/{}/{}/'.format(mod, phase, label)\n",
    "                !mv $path $dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_VAL = [os.path.splitext(path.replace('videos','images'))[0] for path in VIDEO_VAL]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
