{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tqdm/autonotebook/__init__.py:18: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.videodataset import VideoDataset\n",
    "from dataloader.handhygiene import HandHygiene\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter\n",
    "from spatial_transforms import Compose\n",
    "from spatial_transforms import Normalize\n",
    "from spatial_transforms import Scale\n",
    "from spatial_transforms import CenterCrop\n",
    "from spatial_transforms import RandomHorizontalFlip\n",
    "from spatial_transforms import RandomAffine\n",
    "from spatial_transforms import RandomRotation\n",
    "from spatial_transforms import ColorJitter\n",
    "from spatial_transforms import ToTensor #ExtractSkinColor\n",
    "from temporal_transforms import TemporalRandomChoice\n",
    "from temporal_transforms import TemporalRandomCrop\n",
    "from temporal_transforms import LoopPadding, MirrorPadding, MirrorLoopPadding\n",
    "from openpose_transforms import MultiScaleTorsoRandomCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIR='./data/images' #/data/videos\n",
    "CLIP_LEN = 16\n",
    "CLIP_LEN_AUG = CLIP_LEN/2\n",
    "BATCH_SIZE = 16\n",
    "NUM_CLASSES=1\n",
    "NUM_WORKERS=16\n",
    "\n",
    "IMG_SIZE = 224\n",
    "MEAN=[110.63666788, 103.16065604, 96.29023126]\n",
    "STD=[38.7568578, 37.88248729, 40.02898126]\n",
    "#MEAN=[128, 128, 128]\n",
    "#STD=[128, 128, 128]\n",
    "\n",
    "scales=np.linspace(1, 1.75, num=1e3)\n",
    "center=((1+1.75)/2)\n",
    "openpose_transform = {\n",
    "    'train':MultiScaleTorsoRandomCrop(scales, IMG_SIZE),\n",
    "    'val':MultiScaleTorsoRandomCrop(np.linspace(center, center, num=1), IMG_SIZE, centercrop=True)\n",
    "}\n",
    "\n",
    "spatial_transform = {\n",
    "    'train': Compose([Scale(IMG_SIZE),\n",
    "                      CenterCrop(IMG_SIZE),\n",
    "                      RandomHorizontalFlip(),\n",
    "                      ColorJitter(brightness=0.1),\n",
    "                      RandomAffine(5),\n",
    "                      RandomRotation(2.5),\n",
    "                      ToTensor(1), \n",
    "                      Normalize(MEAN, STD)]),\n",
    "    'val': Compose([Scale(IMG_SIZE), \n",
    "                    CenterCrop(IMG_SIZE), \n",
    "                    ToTensor(1), \n",
    "                    Normalize(MEAN, STD)])}\n",
    "\n",
    "temporal_transform = {'train':Compose([\n",
    "                #TemporalRandomCrop(CLIP_LEN_AUG),\n",
    "                TemporalRandomChoice([\n",
    "                    LoopPadding(CLIP_LEN),\n",
    "                    MirrorPadding(CLIP_LEN),\n",
    "                    MirrorLoopPadding(CLIP_LEN)])]),\n",
    "                     'val':LoopPadding(CLIP_LEN)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 54.36it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ./data/images/train video clips: 2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 59.79it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 10.89it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ./data/images/val video clips: 190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 11.76it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.29it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ./data/images/test video clips: 291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.06it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = {\n",
    "    'train': HandHygiene(os.path.join(VIDEO_DIR, 'train'), \n",
    "                          frames_per_clip=CLIP_LEN,\n",
    "                          temporal_transform=temporal_transform['train'],\n",
    "                          openpose_transform=openpose_transform['train'],\n",
    "                          spatial_transform=spatial_transform['train']),\n",
    "    'val': HandHygiene(os.path.join(VIDEO_DIR, 'val'), \n",
    "                        frames_per_clip=CLIP_LEN,\n",
    "                          temporal_transform=temporal_transform['val'],\n",
    "                          openpose_transform=openpose_transform['val'],\n",
    "                        spatial_transform=spatial_transform['val']),\n",
    "    'test': HandHygiene(os.path.join(VIDEO_DIR, 'test'), \n",
    "                        frames_per_clip=CLIP_LEN,\n",
    "                          temporal_transform=temporal_transform['val'],\n",
    "                          openpose_transform=openpose_transform['val'],\n",
    "                        spatial_transform=spatial_transform['val'])\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "     'train': DataLoader(dataset['train'], batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS),\n",
    "    'val': DataLoader(dataset['val'], batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loader = dataloaders['train']\n",
    "for epoch in range(10):\n",
    "    for idx, data in enumerate(tqdm(loader)):\n",
    "        len(loader.dataset[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loader = dataloaders['train']\n",
    "#loader = dataloaders['val']\n",
    "for epoch in range(1):\n",
    "    for idx, data in enumerate(tqdm(loader)):\n",
    "        print('Epoch {}, idx {}'.format(epoch, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader.io.video import read_video\n",
    "video_path='./data/images/val/clean/38_20190119_frames003359'\n",
    "_,_,info = read_video(video_path, 0, None, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def show_dataset(item): # item: C x D x 244 x 244\n",
    "    rgbs = item[0].transpose(0, 1)\n",
    "    flows = item[1].transpose(0, 1)\n",
    "    n = len(rgbs)\n",
    "    shape = np.asarray(flows[0]).shape\n",
    "    rgb = np.hstack((np.asarray(rgb).transpose(1, 2, 0)+1)/2 for rgb in rgbs)\n",
    "    tmp = np.zeros((shape[1], shape[2], 1))\n",
    "    flow = np.hstack((np.dstack((np.asarray(flow).transpose(1, 2, 0), tmp))+1)/2 for flow in flows)\n",
    "\n",
    "    img = np.vstack((rgb, flow))\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(item[2])\n",
    "    \n",
    "i=0\n",
    "phase='train'\n",
    "show_dataset(dataset[phase][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "phase='train'\n",
    "flows_clean_mean=[]\n",
    "flows_clean_std=[]\n",
    "flows_notclean_mean=[]\n",
    "flows_notclean_std=[]\n",
    "for i in tqdm(range(dataset[phase].__len__())):\n",
    "    flow = dataset[phase][i][1]\n",
    "    vidx, _ = dataset[phase].video_clips.get_clip_location(i)\n",
    "    label = dataset[phase].samples[vidx][2]\n",
    "    if label == 0:\n",
    "        flows_clean_mean.append(torch.mean(flow))\n",
    "        flows_clean_std.append(torch.std(flow))\n",
    "    else:\n",
    "        flows_notclean_mean.append(torch.mean(flow))\n",
    "        flows_notclean_std.append(torch.std(flow))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import get_models\n",
    "from train import train\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "i3d_rgb, i3d_flow = get_models(NUM_CLASSES, True, 170, load_pt_weights=True) # unfreeze last mix 170, 152\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    i3d_rgb = nn.DataParallel(i3d_rgb)\n",
    "    i3d_flow = nn.DataParallel(i3d_flow)\n",
    "i3d_rgb.to(device)\n",
    "i3d_flow.to(device)\n",
    "\n",
    "criterion = F.binary_cross_entropy\n",
    "optims={'rgb':None, 'flow':None}\n",
    "schedulers = {'rgb':None, 'flow':None}\n",
    "feature_extract=True\n",
    "\n",
    "def trainable_params(model, mode='rgb'):\n",
    "    params_to_update = model.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\",name)\n",
    "    else:\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(\"\\t\",name)\n",
    "    optims[mode] = optim.SGD(model.parameters(), lr=1e-6, momentum=0.9, weight_decay=1e-7)\n",
    "\n",
    "trainable_params(i3d_rgb, 'rgb')\n",
    "trainable_params(i3d_flow, 'flow')\n",
    "    \n",
    "schedulers['rgb'] = MultiStepLR(optims['rgb'], milestones=[10], gamma=0.1)\n",
    "schedulers['flow'] = MultiStepLR(optims['flow'], milestones=[10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train((i3d_rgb, i3d_flow), dataloaders, optims, criterion, schedulers, device, num_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST DATA SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# for path in sorted(glob('./data/videos/simulate/clean/*.mp4'))[-17:]:\n",
    "#     !mv $path /data/private/minjee-video/handhygiene/data/videos/simulate/test/clean\n",
    "for mod in ['images']:\n",
    "    ext = '.mp4' if mod == 'videos' else ''\n",
    "    for path in sorted(glob('./data/{}/simulate/notclean/*{}'.format(mod, ext)))[-10:]:\n",
    "        path = path.replace('videos', mod)\n",
    "        dst = '/data/private/minjee-video/handhygiene/data/{}/simulate/test/notclean'.format(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for mod in ['images']:\n",
    "    ext = '.mp4' if mod == 'videos' else ''\n",
    "    for label in ['notclean']:\n",
    "        VIDEOS = sorted(glob('./data/{}/simulate/{}/*{}'.format(mod, label, ext)))\n",
    "        #VIDEO_TRAIN, VIDEO_VAL = train_test_split(VIDEOS, test_size=0.2, random_state=42)\n",
    "        VIDEO_DATA = {'train': VIDEO_TRAIN, 'val': VIDEO_VAL}\n",
    "        for phase in ['train', 'val']:\n",
    "            paths = VIDEO_DATA[phase]\n",
    "            for path in paths:\n",
    "                path = path.replace('videos', mod)\n",
    "                dst = './data/{}/simulate/{}/{}/'.format(mod, phase, label)\n",
    "                !mv $path $dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_VAL = [os.path.splitext(path.replace('videos','images'))[0] for path in VIDEO_VAL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "N_epoch = 10\n",
    "\n",
    "loader = dataloaders['train']\n",
    "\n",
    "time_index_all = {}\n",
    "time_loader_all = {}\n",
    "\n",
    "for i in range (2002):\n",
    "    time_index_all[i] = 0\n",
    "for i in range (N_epoch):\n",
    "    time_loader_all[i] = 0\n",
    "\n",
    "\n",
    "for epoch in range(10):\n",
    "    a = []\n",
    "    time_start_loader = time.time()\n",
    "    for idx, data in enumerate(tqdm(loader)):\n",
    "        if idx != 0:\n",
    "            time_end = time.time()\n",
    "            time_idx = time_end-time_start\n",
    "            print (idx, ': %.4f' % (time_idx))\n",
    "            time_index_all[idx-0] += time_idx\n",
    "        time_start = time.time()\n",
    "        \n",
    "        a.append(time)\n",
    "        \n",
    "    time_end_loader = time.time()\n",
    "    time_loader = time_end_loader - time_start_loader\n",
    "    time_loader_all[epoch] = time_loader\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
