{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'torchvision.datasets.video_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-66740f05f52f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVideoClips\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisionDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'torchvision.datasets.video_utils'"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.datasets.video_utils import VideoClips\n",
    "from torchvision.datasets.utils import list_dir\n",
    "from torchvision.datasets.folder import make_dataset\n",
    "from torchvision.datasets.vision import VisionDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'video_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e63e2eef85a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvideo_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'video_utils'"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import video_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class KineticsVideo(VisionDataset):\n",
    "    def __init__(self, root, frames_per_clip, step_between_clips=1):\n",
    "        super(KineticsVideo, self).__init__(root)\n",
    "        extensions = ('avi',)\n",
    "\n",
    "        classes = list(sorted(list_dir(root)))\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        self.samples = make_dataset(self.root, class_to_idx, extensions, is_valid_file=None)\n",
    "        self.classes = classes\n",
    "        video_list = [x[0] for x in self.samples]\n",
    "        self.video_clips = VideoClips(video_list, frames_per_clip, step_between_clips)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.video_clips.num_clips()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video, audio, info, video_idx = self.video_clips.get_clip(idx)\n",
    "        label = self.samples[video_idx][1]\n",
    "\n",
    "        return video, audio, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io, shutil\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tensorboardX import SummaryWriter\n",
    "from dataloader.handhygiene import HandHygiene\n",
    "#from dataloader.i3ddataset import I3DDataset\n",
    "from spatial_transforms import (\n",
    "    Compose, Normalize, Scale, CenterCrop, CornerCrop, MultiScaleCornerCrop,\n",
    "    MultiScaleRandomCrop, RandomHorizontalFlip, RandomAffine, #ExtractSkinColor,\n",
    "    RandomRotation, ColorJitter, ToTensor)\n",
    "from temporal_transforms import (\n",
    "    MirrorPadding, LoopPadding, TemporalBeginCrop, \n",
    "    TemporalRandomCrop, TemporalCenterCrop, TemporalRandomChoice)\n",
    "from openpose_transforms import CropTorso, MultiScaleTorsoRandomCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'i3d'\n",
    "batch_size = 16\n",
    "clip_len = 16\n",
    "sample_duration = clip_len\n",
    "num_classes = 1\n",
    "\n",
    "#torch.manual_seed(100)\n",
    "data_name = 'anesthesia'\n",
    "dataset_path = os.path.join(os.getcwd(), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 224\n",
    "#mean=[110.63666788, 103.16065604, 96.29023126]\n",
    "#std=[38.7568578, 37.88248729, 40.02898126]\n",
    "mean=[128, 128, 128]\n",
    "std=[128, 128, 128]\n",
    "\n",
    "scales=np.linspace(1, 1.75, num=1e3)\n",
    "center=((1+1.75)/2)\n",
    "openpose_transform = {\n",
    "    'train':MultiScaleTorsoRandomCrop(scales, sample_size),\n",
    "    'val':MultiScaleTorsoRandomCrop(np.linspace(center, center, num=1), sample_size, centercrop=True)\n",
    "}\n",
    "spatial_transform = {\n",
    "    'train': Compose([Scale(sample_size),\n",
    "                      CenterCrop(sample_size),\n",
    "                      RandomHorizontalFlip(),\n",
    "                      ColorJitter(brightness=0.1),\n",
    "                      RandomAffine(5),\n",
    "                      RandomRotation(2.5),\n",
    "                      ToTensor(1), \n",
    "                      Normalize(mean, std)]),\n",
    "    'val': Compose([Scale(sample_size), \n",
    "                    CenterCrop(sample_size), \n",
    "                    ToTensor(1), \n",
    "                    Normalize(mean, std)])}\n",
    "temporal_transform = {\n",
    "    'train': TemporalRandomChoice([\n",
    "            TemporalBeginCrop(sample_duration),\n",
    "            MirrorPadding(sample_duration)]),\n",
    "    'val':TemporalBeginCrop(sample_duration)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = {\n",
    "    'train':HandHygiene(dataset_path, split='train', clip_len=clip_len, \n",
    "                        spatial_transform=spatial_transform['train'],\n",
    "                        openpose_transform=openpose_transform['train'],\n",
    "                        temporal_transform=temporal_transform['train'], cropped=False),\n",
    "    'val':HandHygiene(dataset_path, split='val', clip_len=clip_len, \n",
    "                        openpose_transform=openpose_transform['val'],\n",
    "                        spatial_transform=spatial_transform['val'],\n",
    "                        temporal_transform=temporal_transform['val'], cropped=False),\n",
    "    'test':HandHygiene(dataset_path, split='test', clip_len=clip_len, \n",
    "                        openpose_transform=openpose_transform['val'],\n",
    "                        spatial_transform=spatial_transform['val'],\n",
    "                        temporal_transform=temporal_transform['val'], cropped=False)}\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(dataset['train'], batch_size=batch_size, shuffle=True, num_workers=16),\n",
    "    'val': DataLoader(dataset['val'], batch_size=batch_size, shuffle=False, num_workers=16)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import get_models\n",
    "from train import train\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def show_dataset(item): # item: C x D x 244 x 244\n",
    "    rgbs = item[0].transpose(0, 1)\n",
    "    flows = item[1].transpose(0, 1)\n",
    "    n = len(rgbs)\n",
    "    shape = np.asarray(flows[0]).shape\n",
    "    rgb = np.hstack((np.asarray(rgb).transpose(1, 2, 0)+1)/2 for rgb in rgbs)\n",
    "    #gray = np.hstack((np.squeeze(np.asarray(gray))+1)/2 for gray in rgbs)\n",
    "    tmp = np.zeros((shape[1], shape[2], 1))\n",
    "    flow = np.hstack((np.dstack((np.asarray(flow).transpose(1, 2, 0), tmp))+1)/2 for flow in flows)\n",
    "\n",
    "    img = np.vstack((rgb, flow))\n",
    "    plt.figure(figsize=(50, 10))\n",
    "    plt.imshow(img)\n",
    "    #plt.imshow(gray)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    print(item[2])\n",
    "\n",
    "i=1\n",
    "phase='val'\n",
    "show_dataset(dataset[phase].__getitem__(i))\n",
    "print(dataset[phase].__getpath__(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "i3d_rgb, i3d_flow = get_models(num_classes, True, 170, load_pt_weights=True) # unfreeze last mix 170, 152\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    i3d_rgb = nn.DataParallel(i3d_rgb)\n",
    "    i3d_flow = nn.DataParallel(i3d_flow)\n",
    "i3d_rgb.to(device)\n",
    "i3d_flow.to(device)\n",
    "\n",
    "criterion = F.binary_cross_entropy\n",
    "optims={'rgb':None, 'flow':None}\n",
    "schedulers = {'rgb':None, 'flow':None}\n",
    "feature_extract=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trainable_params(model, mode='rgb'):\n",
    "    params_to_update = model.parameters()\n",
    "    print(\"Params to learn:\")\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\",name)\n",
    "    else:\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(\"\\t\",name)\n",
    "    optims[mode] = optim.SGD(model.parameters(), lr=1e-6, momentum=0.9, weight_decay=1e-7)\n",
    "\n",
    "trainable_params(i3d_rgb, 'rgb')\n",
    "trainable_params(i3d_flow, 'flow')\n",
    "    \n",
    "schedulers['rgb'] = MultiStepLR(optims['rgb'], milestones=[10], gamma=0.1)\n",
    "schedulers['flow'] = MultiStepLR(optims['flow'], milestones=[10], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(i3d_rgb, (3, 16, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train((i3d_rgb, i3d_flow), dataloaders, optims, criterion, schedulers, device, num_epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "with open('data/list_simulation.txt', 'r') as f:\n",
    "    data_all = f.readlines()\n",
    "data_all = [os.path.join('./data/images/train/clean/', path.strip('\\n')) for path in data_all]\n",
    "data_train, data_test = train_test_split(data_all[16:], test_size=0.2, random_state=42)\n",
    "for path in data_all[:16]:\n",
    "    src = path+'*'\n",
    "    dir_val = './data/images/test/clean'\n",
    "    #!mv $src $dir_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data_notclean, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 솎아내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phase in ['test']:#['train', 'val', 'test']:\n",
    "    data = sorted(glob('./data/images/{}/notclean/*'.format(phase)))\n",
    "    data = [path.replace('{}/notclean'.format(phase), 'notclean') for path in data]\n",
    "    data_all=[]\n",
    "    for path in data:\n",
    "        augs = sorted(glob(path+'*'))\n",
    "        num=1\n",
    "        if len(augs[::num]) > 30:\n",
    "            num=2\n",
    "        for aug in augs[::num]:\n",
    "            data_all.append(aug)\n",
    "    print(len(data_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in data_all:\n",
    "    !mv $path ./data/images/test/notclean/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "clean = {'train':[], 'val':[], 'test':[]}\n",
    "notclean = {'train':[], 'val':[], 'test':[]}\n",
    "clean_num = {'train':[], 'val':[], 'test':[]}\n",
    "notclean_num = {'train':[], 'val':[], 'test':[]}\n",
    "\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    paths = [path for path in dataset[phase].samples[0] if len(os.path.basename(path).split('_')) == 3]\n",
    "    clean[phase] = [path for path in paths if '/clean/' in path]\n",
    "    notclean[phase] = [path for path in paths if '/notclean/' in path]\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    for f in clean[phase]: \n",
    "        num = len(glob(os.path.join(f, '*.jpg')))\n",
    "        clean_num[phase].append(num)\n",
    "    for f in notclean[phase]: \n",
    "        num = len(glob(os.path.join(f, '*.jpg')))\n",
    "        notclean_num[phase].append(num)\n",
    "\n",
    "print('     ', 'clips', 'images', 'min', 'max')\n",
    "for key, value in clean_num.items():\n",
    "    print(key, len(value), sum(value), min(value), max(value))\n",
    "print('     ', 'clips', 'images', 'min', 'max')\n",
    "for key, value in notclean_num.items():\n",
    "    print(key, len(value), sum(value), min(value), max(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def draw_result(lst_iter, lst_loss, lst_acc, title):\n",
    "    plt.plot(lst_iter, lst_loss, '-b', label='loss')\n",
    "    plt.plot(lst_iter, lst_acc, '-r', label='accuracy')\n",
    "\n",
    "    plt.xlabel(\"n iteration\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(title)\n",
    "\n",
    "    # save image\n",
    "    plt.savefig(title+\".png\")  # should before show method\n",
    "\n",
    "    # show\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def test_draw():\n",
    "    # iteration num\n",
    "    lst_iter = range(100)\n",
    "\n",
    "    # loss of iteration\n",
    "    lst_loss = [0.01 * i + 0.01 * i ** 2 for i in xrange(100)]\n",
    "    # lst_loss = np.random.randn(1, 100).reshape((100, ))\n",
    "\n",
    "    # accuracy of iteration\n",
    "    lst_acc = [0.01 * i - 0.01 * i ** 2 for i in xrange(100)]\n",
    "    # lst_acc = np.random.randn(1, 100).reshape((100, ))\n",
    "    draw_result(lst_iter, lst_loss, lst_acc, \"sgd_method\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
